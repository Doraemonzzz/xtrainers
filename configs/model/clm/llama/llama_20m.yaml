model:
  model_type: causal_lm
  model_name: llama
  model_config:
    vocab_size: 32000
    hidden_size: 512
    intermediate_size: 1408
    num_hidden_layers: 6
    num_attention_heads: 4
    max_position_embeddings: 2048
data:
  tokenizer_path:

  data_path:
